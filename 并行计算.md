参考 https://zhuanlan.zhihu.com/p/355652501

# 一、为什么要并行计算

计算机界有一个著名的定律叫做摩尔定律，是由Intel创始人之一戈登摩尔所提出的。单处理器性能大幅度提升的主要原因之一，是日益增加的集成电路晶体管密度。晶体管的数量增加，功耗和产生的热量也随之增加，当散热能力近乎达到极限时，通过增加单个CPU的性能来换取更高的运算性能已不可取。然而我们可以通过并行的方式来增加晶体管的密度。集成电路制造商的决策是在单个芯片上放置多个相对简单的处理器，这样的集成电路称为多核处理器。**核**已经成为中央处理器或者CPU的代名词。

# 二、如何实现并行计算

一个是任务并行，每个节点执行不同的任务
一个是数据并行，每个节点存储不同的数据
我们主要研究MPI，所以可分为SPMD（单程序多数据）并行和MPMD（多程序多数据）
SPMD：多个计算机集群都运行一个程序，然而各自存储是独立的。
MPMD：相当于各自执行各自的程序。、

# 三、MPI与OpenMPI和MPICH等的关系

MPI是一个信息传递接口，可以理解为一种独立于语言的信息传递标准，而OpenMPI和MPICH等是对这种标准的具体实现，OpenMPI和MPICH等这类库是具体用代码实现MPI。因此我们需要安装OpenMPI和MPICH等来实现MPI。

```
which gcc
which gfortran
```

当显示了gcc和gfortran的路径，即可进行下一步安装

```
wget http://www.mpich.org/static/downloads/3.3.2/mpich-3.3.2.tar.gz 
tar -zxvf mpich-3.3.2.tar.gz #解压下载的压缩包 
cd mpich-3.3.2 #进入解压后的文件夹内 
./configure  --prefix=/usr/local/mpich-3.3.2 
# --prefix这一参数是设置安装的路径，根据需要设置合适的路径即可，但需要记住安装的位置 
make 
make install 
```

环境变量的配置

```
vim ~/.bashrc
export PATH="/usr/local/mpich-3.3.2/bin:$PATH"
source ~/.bashrc
which mpicc
which mpif90
```

显示了路径就说明配置成功了

```
mpirun -np 4 ./hello
```

# 四、MPI的6个基本函数

## 1、MPI_Init

消息传递接口（Message Passing Interface，简称 MPI）中的一个函数，用于初始化 MPI 环境
argc：指向程序启动时的命令行参数数量的指针。这个参数是可选的，可以传递 NULL，如果不需要访问命令行参数的话。
argv：指向程序启动时的命令行参数数组的指针的指针。同样，这个参数也是可选的，可以传递 NULL，如果不需要访问命令行参数的话。

```
call MPI_INIT() #Fortran
MPI_Init(&argc,&argv) #C++/C
```

Fortran版本调用时不用加任何参数，而C++/C需要在main函数中把两个参数传进去

## 2、MPI_Finalize

MPI程序运行结束时都需要调用此函数

```
MPI_Finalize()
```

## 3、COMM_RANKCOMM_RANK

获取当前进程标识

```
int MPI_Comm_Rank(MPI_Comm comm, int *rank)
#include "mpi.h"
int main(int *argc,char* argv[])
{
	int myid;
	MPI_Init(&argc,&argv);
	MPI_Comm_Rank(MPI_COMM_WORLD,&myid);
	if(myid==0)
	{
		printf("Hello!!");
	}
	if(myid==1)
	{
		printf("HI");
	}
	MPI_Finalize();
}
```

### 4、MPI_COMM_SIZE

用于获取给定通信器（communicator）中进程的数量

```
int MPI_Comm_Size(MPI_Comm,int *size)
```

### 5、MPI_SEND

该函数位发送函数，用于进程间发送消息

```
int MPI_Send(type* buf,int count,MPI_Datatype,int dest,int tag,MPI_comm comm)
```

### 6、MPI_RECV

接收函数

```
int MPI_Recv(type* buf,int count,MPI_Datatype,int source,int tag,MPI_Comm comm,MPI_Status *status)
```

source 参数标明了从哪个进程接收消息，最后多一个用于返回状态信息的参数status

## 五、练习

创建一个pi.c文件

```
#include "mpi.h"
#include <stdio.h>
double f(double);
double f(double x)
{
    return (4.0/(1.0+x*x));
}
int main(int argc,char *argv[])
{
    int myid,numprocs;

    MPI_Init(&argc,&argv);

    MPI_Comm_size(MPI_COMM_WORLD,&numprocs);
    MPI_Comm_rank(MPI_COMM_WORLD,&myid);
    printf("Process %d of %d\n", myid, numprocs);

    MPI_Finalize();
}
```

编译

```
mpicc pi.c
```

运行

```
mpicc -o pi.exe pi.c
mpirun -np 6 ./pi.exe
#如果使用c++，则指令mpicc换成为mpiCC或mpic++
```

![运行结果](https://i-blog.csdnimg.cn/direct/55c5c0096944447faed3398c501e4e63.png)

完整代码

```C
#include "mpi.h"
#include <stdio.h>
double f(double);
double f(double x)
{
    return (4.0/(1.0+x*x));
}
int main(int argc,char *argv[])
{
    int myid, numprocs;
    int n, i;
    double mypi, pi;
    double h, sum, x;
    MPI_Init(&argc,&argv);
    MPI_Comm_size(MPI_COMM_WORLD,&numprocs);
    MPI_Comm_rank(MPI_COMM_WORLD,&myid);
    printf("Process %d of %d.\n", myid, numprocs);
    n = 100;
    h = 1.0 / (double) n;
    sum = 0.0;
    for (i = myid + 1; i <= n; i += numprocs)
    {
        x = h * ((double)i - 0.5);
        sum +=f(x);
    }
    mypi = h * sum;
    MPI_Reduce(&mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
    if (myid == 0)
    {
        printf("The result is %.10f.\n",pi);
    }    
    MPI_Finalize();
}
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/640f4f3a18834834a96180fa059e24e7.png)
MPI_Reduce 是 MPI（Message Passing Interface）中的一个函数，用于在多个进程间执行归约（reduction）操作
这行代码的含义如下：

**&mypi**：这是发送缓冲区（send buffer）的地址，它包含了要从每个进程发送到根进程（通常是编号为 0 的进程）的数据。在这个例子中，mypi 是一个 double 类型的变量，它存储了每个进程计算得到的某个值。
**&pi**：这是接收缓冲区（receive buffer）的地址，在根进程中，这个地址用于存储归约操作的结果。在非根进程中，这个参数的值是未定义的，因为非根进程不会接收归约的结果。
**1**：这表示要发送的数据元素的数量。在这个例子中，每个进程只发送一个 double 类型的数据元素。
**MPI_DOUBLE**：这指定了发送数据的类型，即 double 类型。
**MPI_SUM**：这指定了要执行的归约操作，即求和（summation）。MPI 支持多种归约操作，如 MPI_MAX、MPI_MIN、MPI_PROD（乘积）等。
**0**：这是根进程的编号（rank）。在这个例子中，所有进程的数据都将被发送到编号为 0 的进程，并在那里进行求和操作。
**MPI_COMM_WORLD**：这指定了要使用的通信器（communicator）。MPI_COMM_WORLD 是一个预定义的通信器，包含了所有通过 mpirun 或 mpiexec 启动的进程。
当 **MPI_Reduce** 被调用时，每个进程都会将其 mypi 变量的值发送到根进程。在根进程中，这些值会被加起来（因为指定了 MPI_SUM 操作），并将结果存储在 pi 变量中。在非根进程中，pi 变量的值不会被更新。

请注意，在使用 MPI_Reduce 之前，确保所有进程都已经正确地初始化了 MPI 环境（通过调用 MPI_Init），并且在调用 MPI_Reduce 之后，如果需要的话，还应该调用 MPI_Finalize 来清理 MPI 环境。此外，mypi 和 pi 变量应该已经在每个进程中声明并初始化

## 六、对等模式

对等模式即在设计算法时将各进程的地位视为相等。在对等模式中，各进程的功能基本相同，因此使用SPMD（单程序多数据）程序可以更好的表达，使用Jacobi迭代作为范例。
Jacobi迭代算法是用上下左右周围的4个点取平均值来得到新的点。每一轮迭代值都是上一轮的结果。

```
#include <iostream>

int main() {
    const int n = 15;  // 定义常量n
    double A[n][n];    // 声明二维数组A
    double B[n][n];    // 声明二维数组B

    // 初始化数组A
    for (int i = 0; i < n; ++i) {
        for (int j = 0; j < n; ++j) {
            A[i][j] = i + j + 1;  // 假设简单的初始化
        }
    }

    // Jacobi迭代更新B中的值
    for (int i = 1; i < n - 1; ++i) {
        for (int j = 1; j < n - 1; ++j) {
            B[i][j] = 0.25 * (A[i-1][j] + A[i+1][j] + A[i][j-1] + A[i][j+1]);
        }
    }

    // 边界条件赋值
    for (int i = 0; i < n; ++i) {
        B[i][0] = A[i][0];
        B[i][n-1] = A[i][n-1];
        B[0][i] = A[0][i];
        B[n-1][i] = A[n-1][i];
    }

    // 输出B数组
    for (int i = 0; i < n; ++i) {
        for (int j = 0; j < n; ++j) {
            std::cout << B[i][j] << " ";
        }
        std::cout << std::endl;
    }

    return 0;
}
```

### MPI并行计算

A矩阵是16x16的，可以考虑使用4个进程来完成该Jacobi迭代。
而这样又产生了新的问题，即算边界点时，需要相邻进程进程的边界值。比如计算进程0最右侧一列的点时，需要进程1最左侧一列点的值。从图中我们可以看出，进程1和进程2所执行的操作是一样的，都需要从左右两进程接收数据，也需要向左右两侧发送数据。而进程0和进程3则是只用往一侧发送和接收数据。但因为我们所编写的是SPMD程序，所有进程执行的都是同一个代码文件。为了保持一致性和代码的可读性，在每一块的左右边界各加上一列。
c++代码参考：
http://link.zhihu.com/?target=https%3A//www.jianshu.com/p/f567d4e740bb

这个思路就是将16*16的矩阵按列分成四个进程，每个进程负责四列数据的计算，但各进程除了独立计算外还需要进行通信，因为进程负责的矩阵的边缘无法进行计算，所以我们将各进程的矩阵进行扩展，矩阵左右各加一列，则成为了16*6的矩阵，多加的这两列就是为了接受左右进程的数据，还要考虑到0进程和3进程分别没有右边的进程和左边的进程，所以myid>0时更新left，myid<3时更新right。

### 死锁与内存溢出

在阻塞通信时，若MPI_SEND和MPI_RECV顺序不当则会发生死锁的现象，例如进程0和进程1要相互发送消息。而进程0和进程1都先进行接收操作，则两个进程都会等待消息的发送而不会执行下一步的发送操作，此时程序就卡住不动，也就是死锁现象。
如果进程0和进程1同时发消息这可能会导致内存溢出

### 捆绑发送接收

MPI_SEDNRECV和MPI_SENDRECV_REPLACE函数。

### 虚拟进程

MPI_PROC_NULL是一个假想进程，当一个真实进程像一个虚拟进程发送和接受数据时，会立刻执行一个空操作。

## 七、主从模式

从逻辑上规定一个主进程，用于将数据发送各个进程，在收集各进程的结果。

### 矩阵相乘

http://link.zhihu.com/?target=https%3A//www.jianshu.com/p/973e29cbf98d
矩阵a是100*100，矩阵b是100*1
主进程主要干三件事: 定义数据, 发送数据和接收计算结果